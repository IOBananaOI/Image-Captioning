{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libs import"
      ],
      "metadata": {
        "id": "dEOpYeC97Yug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "api_token = {\"username\":\"iobananaoi\",\"key\":\"84cda1d87e243a61b9694f0ef68dc326\"}\n",
        "\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "7xj2FR022YiP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "onR9daREqXci"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "from torchvision import transforms\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "from torchvision import models\n",
        "from functools import reduce\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from torch.nn.utils.rnn import pack_padded_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api = KaggleApi()\n",
        "api.authenticate()"
      ],
      "metadata": {
        "id": "VLKgI_9T3Ljn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d aladdinpersson/flickr8kimagescaptions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98UBGmUzsxbV",
        "outputId": "be3877f7-746d-4bac-d911-b213eb54d7d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading flickr8kimagescaptions.zip to /content\n",
            "100% 1.04G/1.04G [00:58<00:00, 16.0MB/s]\n",
            "100% 1.04G/1.04G [00:58<00:00, 18.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile(\"/content/flickr8kimagescaptions.zip\") as f:\n",
        "    f.extractall()"
      ],
      "metadata": {
        "id": "4NPPtJrI2kHV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essential variables"
      ],
      "metadata": {
        "id": "CC50fNTx7anc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image folder path\n",
        "img_path=\"/content/flickr8k/images/\"\n",
        "\n",
        "# Path to file with captions\n",
        "captions_path=\"/content/flickr8k/captions.txt\"\n",
        "\n",
        "# Encoder output dim\n",
        "enc_dim = 2048\n",
        "\n",
        "# Encoder num of pixels\n",
        "num_pixels = 100\n",
        "\n",
        "# Decoder output dim\n",
        "dec_dim = 50\n",
        "\n",
        "# Attention hidden dim\n",
        "att_dim = 40\n",
        "\n",
        "# Vocab size, including tokens <start>, <end>, <pad>, <unk>\n",
        "vocab_size = 10272\n",
        "\n",
        "# Max caption's length from dataset\n",
        "max_cap_len = 37\n",
        "\n",
        "# Embedding dim\n",
        "emb_dim = 80\n",
        "\n",
        "# Batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Epochs\n",
        "epochs = 10\n",
        "\n",
        "# Learning rate for decoder\n",
        "decoder_lr = 4e-4\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YniHpyb37dJC",
        "outputId": "4ce249aa-b5aa-46ae-f1c2-b5058f7b66fe"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing"
      ],
      "metadata": {
        "id": "3SYm0Wf13phs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wordmap creation"
      ],
      "metadata": {
        "id": "5XoKkqtoUxT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(captions_path, sep=',')\n",
        "captions = dataset[\"caption\"]\n",
        "\n",
        "words = [w for caption in captions for w in caption[:-2].split(\" \")]\n",
        "word_map = {k: v + 1 for v, k in enumerate(words)}\n",
        "word_map['<unk>'] = len(word_map) + 1\n",
        "word_map['<start>'] = len(word_map) + 1\n",
        "word_map['<end>'] = len(word_map) + 1\n",
        "word_map['<pad>'] = 0"
      ],
      "metadata": {
        "id": "iFyhMqyJUw0q"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "S1XMs2Ep6uhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flickr8k(Dataset):\n",
        "    def __init__(self, img_path=img_path, captions_path=captions_path, transform=None, train=True):\n",
        "        self.df = pd.read_csv(captions_path, sep=\",\")\n",
        "        self.df = self.df[:30000] if train else self.df[30000:]\n",
        "\n",
        "        self.transform = transform\n",
        "        self.img_path = img_path\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Read img and corresponding caption\n",
        "        img_name, caption = self.df.iloc[idx][\"image\"], self.df.iloc[idx][\"caption\"][:-2]\n",
        "        img = Image.open(self.img_path+img_name)\n",
        "\n",
        "        # Make transformation of image\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Encode captions\n",
        "        caption = caption.split(\" \")\n",
        "        enc_cap = torch.tensor([word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in caption] + [\n",
        "            word_map['<end>']] + [word_map['<pad>']] * (max_cap_len - len(caption)))\n",
        "\n",
        "        # Find caption length\n",
        "        c_len = torch.tensor(len(caption) + 2)\n",
        "\n",
        "        return img, enc_cap, c_len"
      ],
      "metadata": {
        "id": "PIpd1mav3P19"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((300, 300)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_ds, test_ds = Flickr8k(train=True, transform=transform), Flickr8k(train=False, transform=transform)"
      ],
      "metadata": {
        "id": "RKG_cdvi4VPj"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataLoader"
      ],
      "metadata": {
        "id": "sNLAsoDn6wz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, drop_last=True)\n",
        "test_dl = DataLoader(test_ds, batch_size, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "id": "EJbAi8-w52ps"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_grid(images, caps, nrows=3, ncols=3):\n",
        "    fig, ax = plt.subplots(nrows, ncols, figsize=(20, 13))\n",
        "    for i in range(ncols*nrows):\n",
        "        row = i // ncols\n",
        "        col = i % ncols\n",
        "        img = images[i].cpu().squeeze().permute(1, 2, 0)\n",
        "\n",
        "        ax[row, col].set_title(caps[i])\n",
        "        ax[row, col].imshow(img)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vv6nPyRP-L4q"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model architecture"
      ],
      "metadata": {
        "id": "eHd01Qm7E2aP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        for p in resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        modules = list(resnet.children())[:-2] # Delete last 2 layers as long as we don't have classification problem\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        x = x.permute(0, 2, 3, 1) # [batch_size, 10, 10, 2048]\n",
        "        x = x.view(x.size(0), -1, x.size(-1)) # (batch_size, num_pixels (100), enc_dim (2048))\n",
        "\n",
        "        return x "
      ],
      "metadata": {
        "id": "BVPUBcVeFq4T"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_dim, dec_dim, att_dim):\n",
        "        super(Attention, self).__init__()\n",
        "    \n",
        "        self.enc_att = nn.Linear(enc_dim, att_dim)\n",
        "        self.dec_att = nn.Linear(dec_dim, att_dim)\n",
        "        self.full_att = nn.Linear(att_dim, 1)  \n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, enc_out, dec_hidden):\n",
        "        att1 = self.encoder_att(enc_out)\n",
        "\n",
        "        att2 = self.decoder_att(dec_hidden)  \n",
        "        \n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2) \n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "\n",
        "        att_weighted_encoding = (enc_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, enc_dim)\n",
        "\n",
        "        return att_weighted_encoding, alpha"
      ],
      "metadata": {
        "id": "Ev602eR67Fdr"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.attention = Attention(enc_dim, dec_dim, att_dim)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        self.lstm = nn.LSTMCell(emb_dim + enc_dim, dec_dim)\n",
        "        \n",
        "        self.init_h = nn.Linear(enc_dim, dec_dim)  # hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(enc_dim, dec_dim)  # cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(dec_dim, enc_dim)  # sigmoid-activated gate\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.fc = nn.Linear(dec_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "\n",
        "    def init_hidden_state(self, enc_out):\n",
        "        mean_enc_out = enc_out.mean(dim=1)\n",
        "        h = self.init_h(mean_enc_out)  # (batch_size, dec_dim)\n",
        "        c = self.init_c(mean_enc_out)\n",
        "\n",
        "        return h, c\n",
        "\n",
        "\n",
        "    def forward(self, enc_out, enc_captions, caption_lengths):\n",
        "\n",
        "        # Sort input data by decreasing lengths\n",
        "\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        enc_out = enc_out[sort_ind]\n",
        "        enc_captions = enc_captions[sort_ind]\n",
        "\n",
        "        # Convert captions to embeddings \n",
        "        embeddings = self.embedding(enc_captions)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(enc_out)  # (batch_size, decoder_dim)\n",
        "\n",
        "        # For decoding we don't use <end> token, cause after it was generated our decoding ends\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "        \n",
        "        # Create tensors for word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
        "\n",
        "\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths]) # N_t from the article, means that we take all the images, which cap_len is greater than current time_stamp\n",
        "\n",
        "            attention_weighted_encoding, alpha = self.attention(enc_out[:batch_size_t], h[:batch_size_t])\n",
        "\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
        "\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, enc_captions, decode_lengths, alphas, sort_ind"
      ],
      "metadata": {
        "id": "GL-JXP-IRRyP"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "yEq3Z5-WkyvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(scores, targets, k):\n",
        "    _, ind = scores.topk(k, 1, True, True)\n",
        "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
        "    correct_total = correct.view(-1).float().sum()\n",
        "\n",
        "    return correct_total.item() * (100.0 / batch_size)"
      ],
      "metadata": {
        "id": "pXnZUiT4qyPn"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(enc, dec, dec_optim, dataloader, criterion):\n",
        "    dec.train()\n",
        "\n",
        "    min_loss = 1000\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    enc_state_dict = None\n",
        "    dec_state_dict = None\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        print(f\"======= Epoch: {epoch} =======\")\n",
        "        for i, (imgs, caps, caplens) in enumerate(dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            caps = caps.to(device)\n",
        "            caplens = caplens.to(device)\n",
        "\n",
        "            # Forward prop.\n",
        "            encoded_imgs = enc(imgs)\n",
        "            scores, caps_sorted, decode_lengths, alphas, sort_ind = dec(encoded_imgs, caps, caplens)\n",
        "\n",
        "            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "            targets = caps_sorted[:, 1:]\n",
        "\n",
        "            scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "            targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "            # Evaluate loss\n",
        "            loss = criterion(scores, targets)\n",
        "\n",
        "            # Add doubly stochastic attention regularization\n",
        "            loss += 1. * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "            # Back prop\n",
        "            dec_optim.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            dec_optim.step()\n",
        "\n",
        "            # Metrics\n",
        "            losses.append(loss.item())\n",
        "            top5 = accuracy(targets, scores, 5)\n",
        "            accuracies.append(top5)\n",
        "\n",
        "            if loss.item() < min_loss:\n",
        "                min_loss = loss.item()\n",
        "                enc_state_dict = enc.state_dict()\n",
        "                dec_state_dict = dec.state_dict()\n",
        "\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                print(f\"Batch: {i} Loss: {loss.item()} Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f}\")\n",
        "\n",
        "\n",
        "    return enc_state_dict, dec_state_dict"
      ],
      "metadata": {
        "id": "H1Zy4uyUYzxj"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = Encoder().to(device)\n",
        "\n",
        "dec = Decoder().to(device)\n",
        "dec_optim = torch.optim.Adam(enc.parameters(), lr=decoder_lr)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF6gTsSNk0z4",
        "outputId": "444ef9b6-b3e5-4afd-e0aa-c0c164807c76"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc, dec = train(enc, dec, dec_optim, train_dl, criterion)"
      ],
      "metadata": {
        "id": "C0cG73-OtC5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-13G2xeuxHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}